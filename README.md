

# awesome_crawl [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) 

---

| 爬虫                        | 说明             | 技术栈 |
| ------------------------- | -------------- | -------------- |
| [腾讯新闻](./qq_news/README.md) | 采集所有腾讯新闻的链接和新闻详情        | scrapy,mongo,redis |
| [知乎话题](./zhihu_topic/README.md)                       | 从话题广场出发，先采集子话题ID，再采集ID下所有问题          | scrapy,mongo,redis |
| [微博粉丝](./weibo_fans/README.md)                | 采集大V的所有粉丝          | scrapy,mongo,redis |
| [Tumblr爬虫](./tumblr_spider/README.md) | 下载指定Tumblr博主的资源 | requests,concurrent |
| [妹子图爬虫](./mzitu/README.md) | go语言下载妹子图 | goroutine,goquery |
| [全站爬虫](./kan_sogou/README.md) | 分布式全站爬虫——以"搜狗电视剧"为例 | scrapy,redis，分布式 |



如果能帮助你，那就最好了。欢迎关注公众号：

![](./img/wx.jpg)

